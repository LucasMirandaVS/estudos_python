{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyreUynTw6sd/5bm+Jf8i1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasMirandaVS/estudos_python/blob/main/Data_Manip_Spark_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Dependencies"
      ],
      "metadata": {
        "id": "_H98wn11cT6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65bPDvlVap-7",
        "outputId": "c95de5fb-06cc-4781-d187-b6b584fb515f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "WtzVIOsOceFo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Dataframe Config"
      ],
      "metadata": {
        "id": "ihsC_X22cgWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"ACE Report Transformation\").getOrCreate()\n",
        "\n",
        "# ===== STEP 1 — Load data (CSV recommended, as Spark does not natively read Excel) =====\n",
        "INPUT_PATH  = \"/content/raw_data_test_version.csv\"\n",
        "OUTPUT_PATH = \"ace_report_enriched_sparke.csv\"\n",
        "\n",
        "COL_ENTRY_NUM = \"Entry Summary Number\"\n",
        "COL_LINE_NUM  = \"Entry Summary Line Number\"\n",
        "\n",
        "TARIFF_COLS = [\n",
        "    \"Line Tariff Duty Amount\",\n",
        "    \"Line MPF Amount\",\n",
        "    \"Line HMF Amount\",\n",
        "    \"Antidumping Duty Amount\",\n",
        "    \"Countervailing Duty Amount\",\n",
        "]\n",
        "\n",
        "DROP_EXTRA_COLS = [\n",
        "    \"Line Tariff Goods Value Amount\",\n",
        "    \"Line Tariff Quantity (1)\",\n",
        "    \"Line Tariff UOM (1) Code\",\n",
        "    \"Line Tariff Quantity (2)\",\n",
        "    \"Line Tariff UOM (2) Code\",\n",
        "    \"Line Tariff Quantity (3)\",\n",
        "    \"Line Tariff UOM (3) Code\",\n",
        "]\n",
        "\n",
        "# Read CSV directly into Spark DataFrame\n",
        "raw_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(INPUT_PATH)\n",
        "\n",
        "# ===== STEP 2 — Create \"Entry summary number code\" (clean suffix .0) =====\n",
        "def rm_dotzero(colname: str):\n",
        "    return F.regexp_replace(F.trim(F.col(colname).cast(\"string\")), r\"\\\\.0$\", \"\")\n",
        "\n",
        "transformed_df = raw_df.withColumn(\n",
        "    \"Entry summary number code\",\n",
        "    F.concat_ws(\"-\", rm_dotzero(COL_ENTRY_NUM), rm_dotzero(COL_LINE_NUM))\n",
        ")\n",
        "\n",
        "# Cast tariff columns to double for aggregation\n",
        "for c in TARIFF_COLS:\n",
        "    if c in transformed_df.columns:\n",
        "        transformed_df = transformed_df.withColumn(c, F.col(c).cast(\"double\"))\n",
        "\n",
        "# Cast line number column to numeric for ordering\n",
        "if COL_LINE_NUM in transformed_df.columns:\n",
        "    transformed_df = transformed_df.withColumn(COL_LINE_NUM, F.col(COL_LINE_NUM).cast(\"double\"))\n",
        "\n",
        "# ===== STEP 3 — Group: sum tariff columns by code =====\n",
        "agg_df = transformed_df.groupBy(\"Entry summary number code\").agg(\n",
        "    *[F.sum(F.col(c)).alias(f\"{c}__agg\") for c in TARIFF_COLS]\n",
        ")\n",
        "\n",
        "# ===== STEP 4 — Join totals back =====\n",
        "df = transformed_df.join(agg_df, on=\"Entry summary number code\", how=\"left\")\n",
        "\n",
        "# ===== STEP 5 — Keep totals only on the LAST row per code =====\n",
        "w = Window.partitionBy(\"Entry summary number code\").orderBy(F.col(COL_LINE_NUM).asc())\n",
        "is_last = F.lead(F.col(\"Entry summary number code\")).over(w).isNull()\n",
        "\n",
        "for c in TARIFF_COLS:\n",
        "    df = df.withColumn(\n",
        "        f\"Aggregated {c}\",\n",
        "        F.when(is_last, F.col(f\"{c}__agg\")).otherwise(F.lit(0.0))\n",
        "    )\n",
        "\n",
        "# ===== STEP 6 — Drop original tariff, __agg, and extra columns =====\n",
        "cols_to_drop = [c for c in TARIFF_COLS if c in df.columns] \\\n",
        "             + [f\"{c}__agg\" for c in TARIFF_COLS if f\"{c}__agg\" in df.columns] \\\n",
        "             + [c for c in DROP_EXTRA_COLS if c in df.columns]\n",
        "\n",
        "for c in cols_to_drop:\n",
        "    df = df.drop(c)\n",
        "\n",
        "# ===== STEP 7 — Remove rows where all aggregated values are zero =====\n",
        "aggregated_cols = [f\"Aggregated {c}\" for c in TARIFF_COLS if f\"Aggregated {c}\" in df.columns]\n",
        "if aggregated_cols:\n",
        "    non_zero_cond = None\n",
        "    for c in aggregated_cols:\n",
        "        cond = (F.col(c) != 0)\n",
        "        non_zero_cond = cond if non_zero_cond is None else (non_zero_cond | cond)\n",
        "    df = df.filter(non_zero_cond)\n"
      ],
      "metadata": {
        "id": "kHTyCET2jFI3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Export"
      ],
      "metadata": {
        "id": "IFDFkNiLc_5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== STEP 8 — Write output =====\n",
        "# Write single CSV (coalesce to 1 for convenience; remove if large scale)\n",
        "df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(OUTPUT_PATH)\n",
        "print(f\"Wrote: {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDhnDMIkdB_M",
        "outputId": "b0a60f16-8d33-4a06-8117-0cabdd75dcb8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: ace_report_enriched_sparke.csv\n"
          ]
        }
      ]
    }
  ]
}